import os
import torch
import time
from collections import deque
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM
from langchain.memory import ConversationBufferWindowMemory

def safe_text(text):
    return text.encode("utf-8", "replace").decode("utf-8")

# PDF klasÃ¶rÃ¼ ve dosya listesi
pdf_folder = "birlesik_pdfler2/TYT _ Biyoloji (TYT)"
#pdf_files = sorted([f for f in os.listdir(pdf_folder) if f.endswith(".pdf")])
pdf_files = sorted([f.split("_")[4] for f in os.listdir(pdf_folder) if f.endswith(".pdf")])
#print("[âœ”] PDF dosyalarÄ± bulundu:", pdf_files)

# Qdrant istemcisi ve embedding modeli
#client = QdrantClient(path="qdrant_data")
client = QdrantClient(url="http://localhost:6333")
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)
print("[âœ”] Embedding modeli yÃ¼klendi.")

# Vector store tanÄ±mlarÄ±
main_vector_store = QdrantVectorStore(
    client=client,
    collection_name="TYT _ Biyoloji (TYT)",
    embedding=embeddings,
)

keywords_vector_store = QdrantVectorStore(
    client=client,
    collection_name="TYT _ Biyoloji (TYT)_keywords",
    embedding=embeddings,
)


# LLM
llm = OllamaLLM(
    model="gemma3:12b",
    temperature=0.1,
    top_p=0.9,
    repeat_penalty=1.1,
    num_predict=200,
    streaming=True
)

""""
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY",
    model="google/gemma-3-1b-it",
    streaming=True
)
"""


# Prompt
qa_prompt = ChatPromptTemplate.from_template("""
Sen bir Ã–ÄŸretmenisin. AÅŸaÄŸÄ±daki kurallara gÃ¶re yalnÄ±zca verilen "chat_history" (son 3 userâ€“assistant Ã§ifti) ve "context" (PDF'den Ã§ekilen iÃ§erik) iÃ§indeki bilgilerle cevap ver:

1. EÄŸer soru Ã¶nceki konuÅŸma geÃ§miÅŸine (chat_history) dayalÄ± bir takip sorusuysa:
   a) chat_history veya context iÃ§inde doÄŸrudan cevap buluyorsan, eksiksiz ÅŸekilde yanÄ±tla.
   b) Bulamazsan "Bu konu hakkÄ±nda bilgi sahibi deÄŸilimâ€¦" yaz.

2. EÄŸer soru yeni bir soru (chat_history ile iliÅŸkisi yoksa):
   a) Context iÃ§inde soruya yanÄ±t verebilecek bilgi varsa, oradaki bilgiyi kullanarak cevapla.
   b) Yoksa "Bu konu hakkÄ±nda bilgi sahibi deÄŸilimâ€¦" yaz.

CevabÄ±nÄ± yalnÄ±zca chat_history ve context'e dayandÄ±r. Kendi genel bilgin ya da interneti kullanma.
CevabÄ± TÃ¼rkÃ§e ver.

â€”â€” KonuÅŸma GeÃ§miÅŸi â€”â€”  
{chat_history}

â€”â€” Ä°Ã§erik â€”â€”  
{context}

â€”â€” Soru â€”â€”  
{question}

Cevap:
""")

# HafÄ±za baÅŸlatma
memory = ConversationBufferWindowMemory(
    k=3,
    memory_key="chat_history",
    return_messages=False,
    human_prefix="Ã–ÄŸrenci",
    ai_prefix="Ã–ÄŸretmen"
)

def get_context_from_pdf(pdf_name, question, k):

    results_semantic = main_vector_store.similarity_search_with_score(
        query=question,
        k=k,
        filter=models.Filter(
            must=[models.FieldCondition(
                key="metadata.source",
                match=models.MatchValue(value=pdf_name)
            )]
        )
    )

    keyword_results = keywords_vector_store.similarity_search_with_score(
        query=question,
        k=k,
        filter=models.Filter(
            must=[models.FieldCondition(
                key="metadata.source",
                match=models.MatchValue(value=pdf_name)
            )]
        )
    )
    chunk_nos = [doc.metadata.get("chunk_no") for doc, _ in keyword_results]

    results_keywords = []
    for chunk_no in chunk_nos:
        chunks = main_vector_store.similarity_search_with_score(
            query="",
            k=2,
            filter=models.Filter(
                must=[models.FieldCondition(
                    key="metadata.chunk_no",
                    match=models.MatchValue(value=chunk_no)
                )]
            )
        )
        results_keywords.extend(chunks)

    # Duplicate chunk_no'lardan kaÃ§Ä±nmak iÃ§in tekilleÅŸtirme
    seen_chunk_nos = set()
    unique_docs_with_scores = []
    for doc, score in results_semantic + results_keywords:
        chunk_no = doc.metadata.get("chunk_no")
        if chunk_no not in seen_chunk_nos:
            seen_chunk_nos.add(chunk_no)
            unique_docs_with_scores.append((doc, score))

    # Debug iÃ§in ekrana yazdÄ±rma
    for doc, score in unique_docs_with_scores:
        chunk_no = doc.metadata.get("chunk_no", "Bilinmiyor")
        print(f"* [Chunk #{chunk_no}] [SIM={score:.3f}] {doc.page_content}...\n")


    context_text = "\n---\n".join(safe_text(doc.page_content) for doc, _ in unique_docs_with_scores)
    return context_text

# CLI DÃ¶ngÃ¼sÃ¼
def main():
    print("\n[ğŸ”] Sorgulama BaÅŸladÄ±. Ã‡Ä±kmak iÃ§in `q` yazÄ±nÄ±z.")
    print("[PDF SeÃ§enekleri] :")
    for i, pdf in enumerate(pdf_files):
        print(f"[{i}] {pdf}")


    while True:
        pdf_choice = input("\nPDF dosyasÄ±nÄ± seÃ§in (0-{}): ".format(len(pdf_files) - 1))
        if pdf_choice.isdigit() and 0 <= int(pdf_choice) < len(pdf_files):
            pdf_name = pdf_files[int(pdf_choice)]
            print("seÃ§ilen pdf:", pdf_name)
            break
        else:
            print("âŒ GeÃ§ersiz seÃ§im.")


    while True:
        k = int(input("Benzer chunk sayÄ±sÄ± (0<k<11): "))
        if k > 0 and k < 10:
            break
        else:
            print("âŒ GeÃ§ersiz k deÄŸeri.")

    while True:
        question = input("\nâ“ Soru: ").strip()
        if question.lower() == "q":
            print("ğŸšª Ã‡Ä±kÄ±lÄ±yor...")
            break


        context_text = get_context_from_pdf(pdf_name, question, k)
        chat_history = memory.buffer or ""

        print(chat_history)

        formatted = qa_prompt.format(
            chat_history=chat_history,
            context=context_text,
            question=question
        )

        print("ğŸ§  YanÄ±t:\n")
        response = ""
        for token in llm.stream(formatted):
            print(token, end="", flush=True) # bu ÅŸeklide ollama kullanÄ±mÄ±
            #print(token.content, end="", flush=True)
            response += safe_text(token) # bu ÅŸeklide ollama kullanÄ±mÄ±
            #response += safe_text(token.content)
        print("\n")

        memory.save_context(
            {"input": f"Ã–ÄŸrenci: {question}"},
            {"output": f"Ã–ÄŸretmen: {response}"}
        )

if __name__ == "__main__":
    main()
